{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e57cce3b",
   "metadata": {},
   "source": [
    "# Project Gutenberg Pipeline\n",
    "\n",
    "**Input one Gutenberg ebook page URL to: extract metadata → fetch & clean the text → identify narrative boundaries with AI → save the core literary text.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "564fb76e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI client successfully initialised!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "from urllib import request\n",
    "from helper import (\n",
    "    setup_openai,\n",
    "    fetch_metadata,\n",
    "    save_metadata,\n",
    "    plaintext_url,\n",
    "    locate_anchor,\n",
    "    normalise,\n",
    ")\n",
    "from gutenberg_cleaner import simple_cleaner\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "\n",
    "# Initialise OpenAI client\n",
    "client = setup_openai()\n",
    "print(\"OpenAI client successfully initialised!\")\n",
    "\n",
    "user_input = input(\"Enter the Gutenberg ebook ID: \")\n",
    "gutenberg_url = f\"https://www.gutenberg.org/ebooks/{user_input}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd82e0a1",
   "metadata": {},
   "source": [
    "---\n",
    "## 1 · Metadata\n",
    "\n",
    "Scrape the ebook page and persist a JSON sidecar in `metadata/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a47d393b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "title               : Lincoln's Gettysburg AddressGiven November 19, 1863 on the battlefield near Gettysburg, Pennsylvania, USA\n",
      "author              : Lincoln, Abraham\n",
      "language            : English\n",
      "publication_date    : None\n",
      "ebook_no            : 4\n",
      "subjects            : ['Consecration of cemeteries -- Pennsylvania -- Gettysburg', \"Soldiers' National Cemetery (Gettysburg, Pa.)\", 'Lincoln, Abraham, 1809-1865. Gettysburg address']\n",
      "genre               : Consecration of cemeteries -- Pennsylvania -- Gettysburg\n",
      "source_url          : https://www.gutenberg.org/ebooks/4\n"
     ]
    }
   ],
   "source": [
    "# Fetch all available metadata from the Gutenberg ebook page\n",
    "meta = fetch_metadata(gutenberg_url)\n",
    "\n",
    "if meta.get(\"error\"):\n",
    "    print(f\"Warning: {meta['error']}\")\n",
    "\n",
    "for key, value in meta.items():\n",
    "    print(f\"{key:20s}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a627cf49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata saved → metadata\\4.metadata.json\n"
     ]
    }
   ],
   "source": [
    "# Save metadata to metadata/<ebook_id>.metadata.json\n",
    "saved_path = save_metadata(meta, output_dir=\"metadata\")\n",
    "print(f\"Metadata saved → {saved_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25c568ca",
   "metadata": {},
   "source": [
    "---\n",
    "## 2 · Fetch text & preliminary cleanup\n",
    "\n",
    "The plain-text URL is derived automatically from the ebook ID. `gutenberg_cleaner` strips the standard Gutenberg header/footer boilerplate, then we take a 50 000-character `head` and `tail` slice to feed to the LLM — keeping token usage bounded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "542e907b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching: https://www.gutenberg.org/cache/epub/4/pg4.txt\n",
      "\n",
      "Total cleaned characters: 3,398\n",
      "\n",
      "--- HEAD PREVIEW (first 1000 chars) ---\n",
      "501(c)(3) educational corporation organized under the laws of the\n",
      "state of Mississippi and granted tax exempt status by the Internal\n",
      "Revenue Service. The Foundation’s EIN or federal tax identification\n",
      "number is 64-6221541. Contributions to the Project Gutenberg Literary\n",
      "Archive Foundation are tax deductible to the full extent permitted by\n",
      "U.S. federal laws and your state’s laws.\n",
      "\n",
      "The Foundation’s business office is located at 809 North 1500 West,\n",
      "Salt Lake City, UT 84116, (801) 596-1887. Email contact links and up\n",
      "to date contact information can be found at the Foundation’s website\n",
      "and official page at www.gutenberg.org/contact\n",
      "\n",
      "Section 4. Information about Donations to the Project Gutenberg\n",
      "Literary Archive Foundation\n",
      "\n",
      "Project Gutenberg™ depends upon and cannot survive without widespread\n",
      "public support and donations to carry out its mission of\n",
      "increasing the number of public domain and licensed works that can be\n",
      "freely distributed in machine-readable form accessible \n"
     ]
    }
   ],
   "source": [
    "# Derive plain-text URL from ebook ID and fetch\n",
    "ebook_id = meta[\"ebook_no\"]\n",
    "url = plaintext_url(ebook_id)\n",
    "print(f\"Fetching: {url}\")\n",
    "\n",
    "response = request.urlopen(url)\n",
    "raw = response.read().decode(\"utf-8-sig\")\n",
    "\n",
    "# Strip Gutenberg boilerplate — use START/END markers first, fall back to simple_cleaner\n",
    "_start_pat = re.compile(r\"\\*{3}\\s*START OF THE PROJECT GUTENBERG[^\\n]*\\*{3}\", re.IGNORECASE)\n",
    "_end_pat   = re.compile(r\"\\*{3}\\s*END OF THE PROJECT GUTENBERG[^\\n]*\\*{3}\", re.IGNORECASE)\n",
    "\n",
    "_start_m = _start_pat.search(raw)\n",
    "_end_m   = _end_pat.search(raw)\n",
    "\n",
    "if _start_m and _end_m and _end_m.start() > _start_m.end():\n",
    "    # Reliable path: slice between the two markers\n",
    "    cleaned_text = raw[_start_m.end() : _end_m.start()].strip()\n",
    "    print(\"Cleaned via START/END markers.\")\n",
    "else:\n",
    "    # Fallback for files without standard markers\n",
    "    cleaned_text = simple_cleaner(raw)\n",
    "    print(\"Cleaned via simple_cleaner (no standard markers found).\")\n",
    "\n",
    "# Slice head and tail for LLM boundary detection\n",
    "head = cleaned_text[:50000]\n",
    "tail = cleaned_text[-50000:]\n",
    "\n",
    "print(f\"\\nTotal cleaned characters: {len(cleaned_text):,}\")\n",
    "print(f\"\\n--- HEAD PREVIEW (first 1000 chars) ---\\n{head[:1000]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d886087f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- TAIL PREVIEW (last 1000 chars) ---\n",
      "/donate.\n",
      "\n",
      "Section 5. General Information About Project Gutenberg™ electronic works\n",
      "\n",
      "Professor Michael S. Hart was the originator of the Project\n",
      "Gutenberg™ concept of a library of electronic works that could be\n",
      "freely shared with anyone. For forty years, he produced and\n",
      "distributed Project Gutenberg™ eBooks with only a loose network of\n",
      "volunteer support.\n",
      "\n",
      "Project Gutenberg™ eBooks are often created from several printed\n",
      "editions, all of which are confirmed as not protected by copyright in\n",
      "the U.S. unless a copyright notice is included. Thus, we do not\n",
      "necessarily keep eBooks in compliance with any particular paper\n",
      "edition.\n",
      "\n",
      "Most people start at our website which has the main PG search\n",
      "facility: www.gutenberg.org.\n",
      "\n",
      "This website includes information about Project Gutenberg™,\n",
      "including how to make donations to the Project Gutenberg Literary\n",
      "Archive Foundation, how to help produce our new eBooks, and how to\n",
      "subscribe to our email newsletter to hear about new eBooks.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"--- TAIL PREVIEW (last 1000 chars) ---\\n{tail[-1000:]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af6ddb24",
   "metadata": {},
   "source": [
    "---\n",
    "## 3 · Identify narrative boundaries with AI\n",
    "\n",
    "Six chained LLM calls form two symmetric pipelines (start and end), each following the same **Map → Select → Extract** pattern:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f4e24845",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Segment Analysis:\n",
      "1. [Foundation identification, EIN, tax-deductible statement, and contact information (including business office address and contact webpage)] (EDITORIAL)\n",
      "2. [Section 4. Information about Donations to the Project Gutenberg Literary Archive Foundation] (EDITORIAL)\n",
      "3. [Section 5. General Information About Project Gutenberg™ electronic works] (EDITORIAL)\n"
     ]
    }
   ],
   "source": [
    "# -- Call 1: Start Mapper\n",
    "# Decompose the opening of the text into labelled segments\n",
    "start_mapper_prompt = f\"\"\"\n",
    "<task_objective>\n",
    "Decompose the text into **Structural Segments**.\n",
    "Map the transition from front matter to the literary narrative.\n",
    "</task_objective>\n",
    "\n",
    "<segment_classification_rules>\n",
    "- **EDITORIAL**: content not originating from the author (metadata, forewords, copyright information, transcriber notes, contents pages, prolegomenon, frontispiece, publisher information).\n",
    "- **AUTHORIAL**: content originating from the author (main narrative, chapters, poems, prologues, prefaces, epigraphs, introductions, half title).\n",
    "- **AUTHORIAL (ISLAND)**: Very short authorial fragments immediately followed by EDITORIAL segments.\n",
    "</segment_classification_rules>\n",
    "\n",
    "<mapping_instructions>\n",
    "1. Analyze the text.\n",
    "2. List every distinct heading or section as a numbered segment.\n",
    "3. If a \"Sandwich\" (Authorial -> Editorial -> Authorial) occurs, precisely delineate the breaks.\n",
    "</mapping_instructions>\n",
    "\n",
    "<output_format>\n",
    "Return a numbered list only, for example:\n",
    "1. [Segment Name] (EDITORIAL)\n",
    "2. [Segment Name] (AUTHORIAL - ISLAND)\n",
    "3. [Segment Name] (EDITORIAL)\n",
    "4. [Segment Name] (AUTHORIAL)\n",
    "</output_format>\n",
    "\n",
    "TEXT:\n",
    "{head[:50000]}\n",
    "\"\"\"\n",
    "\n",
    "response = client.responses.create(model=\"gpt-5-mini\", input=start_mapper_prompt)\n",
    "start_segments = response.output_text\n",
    "print(f\"Start Segment Analysis:\\n{start_segments}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cce18e18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target Start Segment:\n",
      "NULL\n"
     ]
    }
   ],
   "source": [
    "# -- Call 2: Start Selector\n",
    "# Pick the single segment where the main narrative begins\n",
    "start_selector_prompt = f\"\"\"\n",
    "<task>\n",
    "Select the \"Narrative Anchor\": the specific segment where the main literary work begins.\n",
    "</task>\n",
    "\n",
    "<input_analysis>\n",
    "{start_segments}\n",
    "</input_analysis>\n",
    "\n",
    "<selection_logic>\n",
    "1. **Continuity Test:** Locate the LAST (EDITORIAL) segment. The (AUTHORIAL) segment immediately following it is the primary candidate.\n",
    "2. **Island Bypass:** If an (AUTHORIAL - ISLAND) appears early but is separated from the main body by more EDITORIAL text, skip it.\n",
    "3. **The Mainland Rule:** Select the segment that initiates the final, unbroken sequence of (AUTHORIAL) content.\n",
    "</selection_logic>\n",
    "\n",
    "<priority_ranking>\n",
    "- Priority 1: First Chapter/Book heading after the final editorial interruption.\n",
    "- Priority 2: Authorial Preface leading directly into narrative segments.\n",
    "- Priority 3: First Authorial segment (if no Editorial segments exist).\n",
    "</priority_ranking>\n",
    "\n",
    "<output_rule>\n",
    "Return ONLY the exact name of the segment. Return 'NULL' if no narrative start is found.\n",
    "</output_rule>\n",
    "\"\"\"\n",
    "\n",
    "response = client.responses.create(model=\"gpt-5-nano\", input=start_selector_prompt)\n",
    "start_segment = response.output_text.strip()\n",
    "print(f\"Target Start Segment:\\n{start_segment}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dfdc13ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Anchor:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# -- Call 3: Start Extractor\n",
    "# Pull verbatim opening lines of the target segment to use as an anchor string\n",
    "start_extractor_prompt = f\"\"\"\n",
    "<task>\n",
    "Extract the exact literal starting string for the segment: '{start_segment}'.\n",
    "</task>\n",
    "\n",
    "<extraction_rules>\n",
    "- **Zero-Tolerance for Hallucination**: Do not summarise or fix typos; precision and accuracy are key.\n",
    "- **Starting Point**: Begin from the first character of the segment's title/heading.\n",
    "- **Length**: Provide exactly the first 5 lines of the text from this segment.\n",
    "- **Precision**: Maintain all original whitespace, capitalisation, and line breaks.\n",
    "</extraction_rules>\n",
    "\n",
    "<uniqueness_verification>\n",
    "Ensure the extracted snippet is unique. If '{start_segment}' appears in a Table of Contents (EDITORIAL), ignore it and find the version that starts the actual (AUTHORIAL) text block.\n",
    "</uniqueness_verification>\n",
    "\n",
    "<output_restriction>\n",
    "Return ONLY the raw text. No labels. No preamble.\n",
    "</output_restriction>\n",
    "\n",
    "TEXT:\n",
    "{head[:15000]}\n",
    "\"\"\"\n",
    "\n",
    "response = client.responses.create(model=\"gpt-5-mini\", input=start_extractor_prompt)\n",
    "start_anchor = response.output_text.strip()\n",
    "print(f\"Start Anchor:\\n{start_anchor}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d91cc475",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End Segment Analysis:\n",
      "1. Transition: end of literary narrative → start of Project Gutenberg back‑matter (EDITORIAL)\n",
      "2. Project Gutenberg Literary Archive Foundation — tax‑exempt status and EIN (501(c)(3)) (EDITORIAL)\n",
      "3. Foundation contact information and business office address (EDITORIAL)\n",
      "4. Section 4: Information about Donations to the Project Gutenberg Literary Archive Foundation — overview / dependence on public support (EDITORIAL)\n",
      "5. Section 4: Importance of small donations and fundraising rationale (EDITORIAL)\n",
      "6. Section 4: Compliance with state charity solicitation laws and restrictions on soliciting in noncompliant states (EDITORIAL)\n",
      "7. Section 4: Policy on accepting unsolicited donations from noncompliant states (EDITORIAL)\n",
      "8. Section 4: International donations and tax treatment disclaimer (EDITORIAL)\n",
      "9. Section 4: Directions to Project Gutenberg donation web pages and accepted donation methods (EDITORIAL)\n",
      "10. Section 5: General Information About Project Gutenberg™ electronic works — origin and mission (EDITORIAL)\n",
      "11. Section 5: Professor Michael S. Hart as originator and historical note (EDITORIAL)\n",
      "12. Section 5: How Project Gutenberg eBooks are created from printed editions and copyright confirmation practice (EDITORIAL)\n",
      "13. Section 5: Main website, search facility, and links for more information (EDITORIAL)\n"
     ]
    }
   ],
   "source": [
    "# -- Call 4: End Mapper\n",
    "# Decompose the closing of the text into labelled segments\n",
    "end_mapper_prompt = f\"\"\"\n",
    "<task_objective>\n",
    "Decompose the text into **Structural Segments**.\n",
    "Map the transition from the literary narrative to the back matter.\n",
    "Identify every distinct section in the final 25,000 characters.\n",
    "</task_objective>\n",
    "\n",
    "<segment_classification_rules>\n",
    "- **AUTHORIAL**: content originating from the author (main narrative, afterwords, epilogues, authorial postscripts, authorial appendices).\n",
    "- **AUTHORIAL (ISLAND)**: Short authorial fragments (e.g., \"The End\") that are followed by EDITORIAL text.\n",
    "- **EDITORIAL**: content not originating from the author (non-authorial appendices, glossaries, index, endnotes, acknowledgments, transcriber notes, metadata).\n",
    "</segment_classification_rules>\n",
    "\n",
    "<output_format>\n",
    "Return a numbered list of the final segments:\n",
    "1. [Segment Name] (AUTHORIAL)\n",
    "2. [Segment Name] (AUTHORIAL - ISLAND)\n",
    "3. [Segment Name] (EDITORIAL)\n",
    "</output_format>\n",
    "\n",
    "TEXT:\n",
    "{tail[-25000:]}\n",
    "\"\"\"\n",
    "\n",
    "response = client.responses.create(model=\"gpt-5-mini\", input=end_mapper_prompt)\n",
    "end_segments = response.output_text\n",
    "print(f\"End Segment Analysis:\\n{end_segments}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a1f07447",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target End Segment:\n",
      "Transition: end of literary narrative → start of Project Gutenberg back‑matter (EDITORIAL)\n"
     ]
    }
   ],
   "source": [
    "# -- Call 5: End Selector\n",
    "# Identify the last segment that belongs to the author\n",
    "end_selector_prompt = f\"\"\"\n",
    "<task>\n",
    "Based on the segment analysis, identify the **Exit Anchor**: the specific segment where the narrative officially concludes.\n",
    "</task>\n",
    "\n",
    "<input_analysis>\n",
    "{end_segments}\n",
    "</input_analysis>\n",
    "\n",
    "<selection_logic>\n",
    "1. **The Exit Test:** Identify the LAST segment labeled (AUTHORIAL) or (AUTHORIAL - ISLAND) that occurs before the final block of (EDITORIAL) text begins.\n",
    "2. **Postscript Rule:** If a segment is an \"Epilogue\" or \"Author's Note,\" it should be included as the exit anchor unless it is separated from the story by a significant EDITORIAL block.\n",
    "3. **The Finality Rule:** We want the very last bit of text that came from the author's pen.\n",
    "</selection_logic>\n",
    "\n",
    "<output_rule>\n",
    "Return ONLY the exact name of the segment. No extra text.\n",
    "</output_rule>\n",
    "\"\"\"\n",
    "\n",
    "response = client.responses.create(model=\"gpt-5-nano\", input=end_selector_prompt)\n",
    "end_segment = response.output_text.strip()\n",
    "print(f\"Target End Segment:\\n{end_segment}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "779db371",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End Anchor:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# -- Call 6: End Extractor\n",
    "# Pull verbatim closing lines of the exit segment to use as an anchor string\n",
    "end_extractor_prompt = f\"\"\"\n",
    "<task>\n",
    "Extract the exact literal CLOSING string for the segment: '{end_segment}'.\n",
    "</task>\n",
    "\n",
    "<extraction_rules>\n",
    "- **Zero-Tolerance for Hallucination**: Return only what is present in the TEXT.\n",
    "- **Precision**: You are looking for the LAST 5 lines of the '{end_segment}' section.\n",
    "- **Verbatim**: Maintain all original whitespace, capitalisation, and punctuation.\n",
    "- **Constraint**: Do NOT include any text from the EDITORIAL segments that follow it.\n",
    "</extraction_rules>\n",
    "\n",
    "<output_restriction>\n",
    "Return ONLY the raw text. No labels. No preamble.\n",
    "</output_restriction>\n",
    "\n",
    "TEXT:\n",
    "{tail[-25000:]}\n",
    "\"\"\"\n",
    "\n",
    "response = client.responses.create(model=\"gpt-5-mini\", input=end_extractor_prompt)\n",
    "end_anchor = response.output_text.strip()\n",
    "print(f\"End Anchor:\\n{end_anchor}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0811f13a",
   "metadata": {},
   "source": [
    "---\n",
    "## 4 · Extract & save core text\n",
    "\n",
    "Locate the two anchor strings in the cleaned text, slice out everything between them, and write it to `core_txts/<ebook_id>_clean.txt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9b2996e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start: index=-1, length=0\n",
      "End:   index=-1,   length=0\n",
      "\n",
      "Normalised start: ''\n",
      "Normalised end:   ''\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Anchor matching failed — start_idx=-1, end_idx=-1.\nRe-run the LLM calls or inspect the anchor strings above.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m     15\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEND   → \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mrepr\u001b[39m(text_core[-\u001b[32m80\u001b[39m:])\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m     18\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mAnchor matching failed — start_idx=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstart_idx\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, end_idx=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mend_idx\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     19\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mRe-run the LLM calls or inspect the anchor strings above.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     20\u001b[39m     )\n",
      "\u001b[31mRuntimeError\u001b[39m: Anchor matching failed — start_idx=-1, end_idx=-1.\nRe-run the LLM calls or inspect the anchor strings above."
     ]
    }
   ],
   "source": [
    "# Locate both anchor strings in the cleaned text\n",
    "start_idx, start_len = locate_anchor(cleaned_text, start_anchor, is_start=True)\n",
    "end_idx,   end_len   = locate_anchor(cleaned_text, end_anchor,   is_start=False)\n",
    "\n",
    "print(f\"Start: index={start_idx}, length={start_len}\")\n",
    "print(f\"End:   index={end_idx},   length={end_len}\")\n",
    "print()\n",
    "print(f\"Normalised start: {repr(normalise(start_anchor)[:80])}\")\n",
    "print(f\"Normalised end:   {repr(normalise(end_anchor)[-80:])}\")\n",
    "\n",
    "if start_idx != -1 and end_idx != -1:\n",
    "    text_core = cleaned_text[start_idx : end_idx + end_len]\n",
    "    print(f\"\\nExtraction successful — {len(text_core):,} characters\")\n",
    "    print(f\"\\nSTART → {repr(text_core[:80])}\")\n",
    "    print(f\"END   → {repr(text_core[-80:])}\")\n",
    "else:\n",
    "    raise RuntimeError(\n",
    "        f\"Anchor matching failed — start_idx={start_idx}, end_idx={end_idx}.\\n\"\n",
    "        \"Re-run the LLM calls or inspect the anchor strings above.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3739a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save core text to core_txts/<ebook_id>_clean.txt\n",
    "output_dir = \"core_txts\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "file_path = os.path.join(output_dir, f\"{ebook_id}_clean.txt\")\n",
    "with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(text_core)\n",
    "\n",
    "print(f\"Core text saved → {file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b20234fc",
   "metadata": {},
   "source": [
    "---\n",
    "## 5 · NLTK demo\n",
    "\n",
    "A quick demonstration of NLTK's text-analysis tools on `text_core`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba2dc2d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download(\"punkt_tab\", quiet=True)\n",
    "\n",
    "# Tokenise core text into a word list\n",
    "tokens = word_tokenize(text_core)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aaf830c",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46290257",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6487fc90",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c36805",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrap tokens in NLTK Text for corpus-level analysis\n",
    "nltk_text = nltk.Text(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "649006ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frequently co-occurring word pairs\n",
    "nltk_text.collocations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ce9605",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contextual occurrences of a word — change to any word in the text\n",
    "nltk_text.concordance(\"said\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25fd6f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of key words across the text — edit list to suit the book\n",
    "nltk_text.dispersion_plot([\"he\", \"she\", \"him\", \"her\", \"his\", \"hers\", \"himself\", \"herself\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c82ce9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
